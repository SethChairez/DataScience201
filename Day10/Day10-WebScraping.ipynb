{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6d35f17b-9198-46d0-817e-73ef34eea956",
   "metadata": {},
   "source": [
    "---\n",
    "format:\n",
    "    html: \n",
    "        theme: minty\n",
    "        fontsize: 1.1em\n",
    "        page-layout: article\n",
    "        mermaid:\n",
    "            theme: default\n",
    "\n",
    "toc: true\n",
    "toc-depth: 5\n",
    "toc-expand: 2\n",
    "editor: visual\n",
    "warning: false\n",
    "error: false\n",
    "code-overflow: wrap\n",
    "eval: true\n",
    "\n",
    "title: \"Intermediate Data Science\"\n",
    "subtitle: \"Web Scraping\"\n",
    "author: \"Joanna Bieri <br> DATA201\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c8ad4-4b58-43a8-91b7-f5c0ae5b7386",
   "metadata": {},
   "source": [
    "## Important Information\n",
    "\n",
    "- Email: [joanna_bieri@redlands.edu](mailto:joanna_bieri@redlands.edu)\n",
    "- Office Hours take place in Duke 209 -- [Office Hours Schedule](https://joannabieri.com/schedule.html)\n",
    "- [Class Website](https://joannabieri.com/data201.html)\n",
    "- [Syllabus](https://joannabieri.com/data201/IntermediateDataScience.pdf)\n",
    "\n",
    "## Web Scraping\n",
    "\n",
    "**Web scraping** is the process of automatically extracting data from websites using code. It often involves sending HTTP requests, retrieving HTML content, and parsing it to collect specific information such as text, images, or structured data. Web scraping can be useful for research, business intelligence, price comparison, academic projects, or building datasets that are not otherwise publicly available in a structured form.\n",
    "\n",
    "## Ethical Issues\n",
    "\n",
    "There are some things to be aware of before you start scraping data from the web. \n",
    "\n",
    "- Some data is private or protected. Just because you have access to a websites data doesn't mean you are allowed to scrape it. For example, when you log into Facebook or another social media site, you are granted special access to data about your connected people. It is unethical to use that access to scrape their private data!\n",
    "\n",
    "- Some websites have rules against scraping and will cut of service to users who are clearly scraping data. How do they know? Web scrapers access the website very differently that regular users. If they site has a policy about scraping data then you should follow it and/or content them about getting the data if you have a true academic interest in the data.\n",
    "\n",
    "- The line between web scraping and plagiarism can be very blurry. Make sure that you are citing where your data comes from AND not just reproducing the data exactly. Always citing the source of your data and make sure you are doing something new with it.\n",
    "\n",
    "- Ethics are different depending on if you are using the data for a personal project (eg. you just want to check scores for your favorite team daily and print the stuff you care about) vs if you are using the project for your business or website (eg. publishing information to drive clicks to your site/video/account or making money from the data you collect). In the later case it is EXTRA important to respect the original owner of the data. Drive web traffic back to their site, check with them about using their data, etc.\n",
    "\n",
    "The Ethical Scraper (from https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01):\n",
    "\n",
    "I, the web scraper will live by the following principles:\n",
    "\n",
    "- If you have a public API that provides the data I’m looking for, I’ll use it and avoid scraping all together.\n",
    "- I will always provide a User Agent string that makes my intentions clear and provides a way for you to contact me with questions or concerns.\n",
    "- I will request data at a reasonable rate. I will strive to never be confused for a DDoS attack.\n",
    "- I will only save the data I absolutely need from your page. If all I need it OpenGraph meta-data, that’s all I’ll keep.\n",
    "- I will respect any content I do keep. I’ll never pass it off as my own.\n",
    "- I will look for ways to return value to you. Maybe I can drive some (real) traffic to your site or credit you in an article or post.\n",
    "- I will respond in a timely fashion to your outreach and work with you towards a resolution.\n",
    "- I will scrape for the purpose of creating new value from the data, not to duplicate it.\n",
    "\n",
    "## Basics of HTML for Web Scraping\n",
    "\n",
    "Web scraping relies on understanding the structure of **HTML (HyperText Markup Language)**, since most web pages present their content using HTML tags. Each page is built from a tree-like structure called the **DOM (Document Object Model)**, which organizes elements hierarchically.\n",
    "\n",
    "## Key HTML Elements\n",
    "- **`<html>`**: The root element of an HTML page.  \n",
    "- **`<head>`**: Contains metadata (title, links to CSS/JS).  \n",
    "- **`<body>`**: Holds the visible content of the page.  \n",
    "\n",
    "## Common Tags\n",
    "- **Headings (`<h1>`, `<h2>`, ... `<h6>`)**: Define section titles.  \n",
    "- **Paragraph (`<p>`)**: Holds blocks of text.  \n",
    "- **Links (`<a href=\"...\">`)**: Anchor tags contain hyperlinks.  \n",
    "- **Images (`<img src=\"...\">`)**: Embed images via a source URL.  \n",
    "- **Lists (`<ul>`, `<ol>`, `<li>`)**: Ordered and unordered lists.  \n",
    "- **Tables (`<table>`, `<tr>`, `<td>`)**: Represent structured tabular data.  \n",
    "- **Divisions (`<div>`)**: Generic container often used for layout.  \n",
    "- **Spans (`<span>`)**: Inline container for styling or grouping.  \n",
    "\n",
    "## Attributes\n",
    "HTML tags often include **attributes** that provide additional information:\n",
    "- **`id`**: Unique identifier for an element.  \n",
    "- **`class`**: Groups elements with the same style or purpose.  \n",
    "- **`href`** (in `<a>`): Specifies the destination URL.  \n",
    "- **`src`** (in `<img>`): Provides the image source.  \n",
    "\n",
    "## Importance for Web Scraping\n",
    "- Scraping tools (like **BeautifulSoup**, **lxml**, or **Selenium**) locate elements by tags, attributes, or text.  \n",
    "- Common methods include selecting by `id`, `class`, or tag type.  \n",
    "- Understanding the **nested structure** of HTML helps identify where the target data resides in the DOM.  \n",
    "\n",
    "**Example Snippet:**\n",
    "```html\n",
    "<div class=\"product\">\n",
    "  <h2 class=\"title\">Book Title</h2>\n",
    "  <span class=\"price\">$19.99</span>\n",
    "</div>\n",
    "```\n",
    "## Challenges in Creating Web Scraping Code\n",
    "\n",
    "While web scraping is a powerful technique for gathering data, developers often face several significant challenges when implementing it:\n",
    "\n",
    "### 1. Changing Website Structures\n",
    "- Websites frequently update their HTML layout or CSS classes.\n",
    "- Even small changes (e.g., renaming a `class` or moving content into a new `<div>`) can break scraping scripts.\n",
    "- This requires ongoing maintenance and monitoring.\n",
    "\n",
    "### 2. JavaScript-Rendered Content\n",
    "- Many modern sites load data dynamically using JavaScript (AJAX requests, React, Angular, Vue).\n",
    "- Static HTML parsers (like BeautifulSoup) cannot capture this directly.\n",
    "- Developers may need to use tools like **Selenium**, **Playwright**, or network traffic analysis to retrieve dynamic content.\n",
    "\n",
    "### 3. Anti-Scraping Measures\n",
    "- Websites often deploy techniques to detect and block scrapers:\n",
    "  - CAPTCHAs\n",
    "  - Rate limiting\n",
    "  - IP blocking or blacklisting\n",
    "  - Bot detection systems\n",
    "- Workarounds may involve rotating proxies, adding delays, or mimicking real browser behavior.\n",
    "\n",
    "### 4. Data Quality and Cleaning\n",
    "- Extracted data is often messy (inconsistent formats, missing values, duplicates).\n",
    "- Developers must spend significant time cleaning, normalizing, and validating scraped data before analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf096b4a-cb9c-4f53-83e9-2a27e9604776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic package imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.defaule = 'colab'\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f247a1dd-199f-4044-bc8b-0724f58e8240",
   "metadata": {},
   "source": [
    "## Start with the URL\n",
    "\n",
    "The first thing you should inspect when scraping code is the website URL:\n",
    "\n",
    "    https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\n",
    "\n",
    "The URL has two major parts:\n",
    "\n",
    "- The base: https://realpython.github.io\n",
    "- The path: /fake-jobs/jobs/senior-python-developer-0.html\n",
    "\n",
    "The URL might also have query or pagination information included.\n",
    "\n",
    "### 1. Pagination in URLs\n",
    "Pagination often appears when content is split across multiple pages.\n",
    "\n",
    "**Examples:**\n",
    "- `https://example.com/products?page=1`\n",
    "- `https://example.com/products?page=2`\n",
    "- `https://example.com/articles?page=5&sort=latest`\n",
    "- `https://shop.example.com/category/shoes?p=3`\n",
    "\n",
    "Here, the parameter `page` (or sometimes `p`) changes to load different parts of the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Query Parameters in URLs\n",
    "Query parameters are key–value pairs that appear after a **`?`** in the URL. Multiple parameters are separated by **`&`**.\n",
    "\n",
    "**Examples:**\n",
    "- `https://example.com/search?q=laptop`\n",
    "- `https://example.com/search?q=laptop&sort=price_asc`\n",
    "- `https://example.com/api/items?category=books&limit=20&page=2`\n",
    "- `https://news.example.com/archive?year=2023&month=10`\n",
    "\n",
    "\n",
    "## Developer Tools\n",
    "\n",
    "The next thing you want to do is inspect the website with your browsers developer tools. Developer tools let you see the HTML code that the website uses. \n",
    "\n",
    "### Opening Developer Tools\n",
    "- **Mac:**  \n",
    "  - Press `Cmd + Option + I`  \n",
    "  - Or right-click on the page and choose **Inspect**\n",
    "- **Windows:**  \n",
    "  - Press `Ctrl + Shift + I` or `F12`  \n",
    "  - Or right-click on the page and choose **Inspect**\n",
    "- **Linux:**  \n",
    "  - Press `Ctrl + Shift + I` or `F12`  \n",
    "  - Or right-click on the page and choose **Inspect**\n",
    "\n",
    "---\n",
    "\n",
    "Try this! Navigate to https://realpython.github.io/fake-jobs/ and look at the code.\n",
    "\n",
    "Click on the drop down arrows to investigate the parts of the code. The browser should highlight the parts of the website that the code controls when you hover over the code. What do you notice about the organization of this site?\n",
    "\n",
    "## Scrape the HTML Content\n",
    "\n",
    "Install the requests package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d3d44e-e0f3-47d9-aea4-1f5936d010a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0977fea-88c0-448d-93d8-bdf5de158321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge -y requests  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fefbd-74fb-408b-9a96-921601e753c8",
   "metadata": {},
   "source": [
    "When we call `requests.get()` it grabs whatever information the website sends back. Sometimes this is really straightforward and sometimes you have to deal with issues like JavaScript code and CAPTCHA's.\n",
    "\n",
    "### Static vs. Dynamic Websites in Web Scraping\n",
    "\n",
    "When scraping websites, it’s important to know whether the site is **static** or **dynamic**, since this affects how data is loaded and how it can be extracted.\n",
    "\n",
    "\n",
    "### Static Websites\n",
    "- **Content delivery:** HTML content is fully loaded when the page is requested from the server.  \n",
    "- **Scraping approach:** The desired data is usually visible in the page’s source code and can be extracted with tools like **requests + BeautifulSoup**.  \n",
    "- **Example:** A blog where all posts are present in the HTML file.  \n",
    "- **Advantage:** Simple and predictable structure, easier to scrape.  \n",
    "\n",
    "\n",
    "### Dynamic Websites\n",
    "- **Content delivery:** The initial HTML may be minimal, and content is loaded later using JavaScript (e.g., AJAX, React, Angular).  \n",
    "- **Scraping approach:** The target data may not appear in the raw HTML source; instead it must be captured by:  \n",
    "  - Executing JavaScript (using **Selenium**, **Playwright**, or **Puppeteer**)  \n",
    "  - Inspecting the **Network panel** for API requests returning JSON data  \n",
    "- **Example:** An e-commerce site that loads more products as you scroll.  \n",
    "- **Challenge:** Requires more advanced tools and often more computing resources.  \n",
    "\n",
    "\n",
    "**In scraping practice:** Always check whether the content is present in the HTML source or if it only appears after interaction/JavaScript execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6e8f394-d9f4-4f6b-a49d-fd231fde61e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='realpyth%20on.github.io', port=443): Max retries exceeded with url: /fake-jobs/ (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'realpyth%20on.github.io'. (_ssl.c:1000)\")))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:652\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 652\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[1;32m    653\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    654\u001b[0m     cert_reqs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_reqs,\n\u001b[1;32m    655\u001b[0m     ssl_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version,\n\u001b[1;32m    656\u001b[0m     ssl_minimum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_minimum_version,\n\u001b[1;32m    657\u001b[0m     ssl_maximum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_maximum_version,\n\u001b[1;32m    658\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs,\n\u001b[1;32m    659\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir,\n\u001b[1;32m    660\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_data,\n\u001b[1;32m    661\u001b[0m     cert_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file,\n\u001b[1;32m    662\u001b[0m     key_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_file,\n\u001b[1;32m    663\u001b[0m     key_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_password,\n\u001b[1;32m    664\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname_rm_dot,\n\u001b[1;32m    665\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context,\n\u001b[1;32m    666\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[1;32m    667\u001b[0m     assert_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_hostname,\n\u001b[1;32m    668\u001b[0m     assert_fingerprint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_fingerprint,\n\u001b[1;32m    669\u001b[0m )\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:805\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    803\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 805\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    806\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    807\u001b[0m     keyfile\u001b[38;5;241m=\u001b[39mkey_file,\n\u001b[1;32m    808\u001b[0m     certfile\u001b[38;5;241m=\u001b[39mcert_file,\n\u001b[1;32m    809\u001b[0m     key_password\u001b[38;5;241m=\u001b[39mkey_password,\n\u001b[1;32m    810\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39mca_certs,\n\u001b[1;32m    811\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39mca_cert_dir,\n\u001b[1;32m    812\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39mca_cert_data,\n\u001b[1;32m    813\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    814\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    815\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[1;32m    816\u001b[0m )\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/util/ssl_.py:465\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/util/ssl_.py:509\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[1;32m    456\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    457\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[1;32m    458\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[1;32m    459\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[1;32m    460\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    461\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    462\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[1;32m    463\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/ssl.py:1042\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1042\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/ssl.py:1320\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'realpyth%20on.github.io'. (_ssl.c:1000)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[0;31mSSLError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'realpyth%20on.github.io'. (_ssl.c:1000)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    590\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    591\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    593\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    594\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    595\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    596\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    599\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    600\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    844\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    845\u001b[0m )\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='realpyth%20on.github.io', port=443): Max retries exceeded with url: /fake-jobs/ (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'realpyth%20on.github.io'. (_ssl.c:1000)\")))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://realpyth on.github.io/fake-jobs/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m URL_dynamic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://en.wikipedia.org/wiki/Cat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m page \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(URL)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# This prints out  the whole website code for URL\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(page\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/adapters.py:620\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='realpyth%20on.github.io', port=443): Max retries exceeded with url: /fake-jobs/ (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'realpyth%20on.github.io'. (_ssl.c:1000)\")))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"https://realpyth on.github.io/fake-jobs/\"\n",
    "URL_dynamic = \"https://en.wikipedia.org/wiki/Cat\"\n",
    "\n",
    "page = requests.get(URL)\n",
    "\n",
    "# This prints out  the whole website code for URL\n",
    "print(page.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615122b-5ef3-4e91-9055-19c202c3e1e7",
   "metadata": {},
   "source": [
    "## We will start with a Static Example\n",
    "\n",
    "## Parse the HTML code\n",
    "\n",
    "Now that you have the code as one big string in page.text you need a way to parse it. Beautiful Soup is a Python library for parsing structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62526fff-ca85-4e56-8e57-59eca054f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad614e2e-74c0-472f-bfdd-a949050f63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Example HTML\n",
    "html = \"\"\"\n",
    "<div class=\"product\" id=\"p1\">\n",
    "  <h2 class=\"title\">Book One</h2>\n",
    "  <span class=\"price\">$19.99</span>\n",
    "  <a href=\"https://example.com/book1\" class=\"buy-link\">Buy</a>\n",
    "</div>\n",
    "<div class=\"product\" id=\"p2\">\n",
    "  <h2 class=\"title\">Book Two</h2>\n",
    "  <span class=\"price\">$24.99</span>\n",
    "  <a href=\"https://example.com/book2\" class=\"buy-link\">Buy</a>\n",
    "</div>\n",
    "<img src=\"cover1.jpg\" alt=\"Cover 1\">\n",
    "<img src=\"cover2.jpg\" alt=\"Cover 2\">\n",
    "\"\"\"\n",
    "\n",
    "# Parse HTML\n",
    "example_soup = BeautifulSoup(html, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e982e14b-9cd1-4762-bd09-17c4d7e66086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2 class=\"title\">Book One</h2>\n",
      "h2\n",
      "{'class': ['title']}\n",
      "Book One\n",
      "--------------------\n",
      "<h2 class=\"title\">Book Two</h2>\n",
      "h2\n",
      "{'class': ['title']}\n",
      "Book Two\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Find by tag name\n",
    "info = example_soup.find_all(\"h2\")\n",
    "for i in info:\n",
    "    print(i)\n",
    "    print(i.name)\n",
    "    print(i.attrs)\n",
    "    print(i.text)\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "576c834e-5df7-46da-b0d4-05170bf44ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"product\" id=\"p1\">\n",
      "<h2 class=\"title\">Book One</h2>\n",
      "<span class=\"price\">$19.99</span>\n",
      "<a class=\"buy-link\" href=\"https://example.com/book1\">Buy</a>\n",
      "</div>\n",
      "--------------------\n",
      "[<div class=\"product\" id=\"p1\">\n",
      "<h2 class=\"title\">Book One</h2>\n",
      "<span class=\"price\">$19.99</span>\n",
      "<a class=\"buy-link\" href=\"https://example.com/book1\">Buy</a>\n",
      "</div>, <div class=\"product\" id=\"p2\">\n",
      "<h2 class=\"title\">Book Two</h2>\n",
      "<span class=\"price\">$24.99</span>\n",
      "<a class=\"buy-link\" href=\"https://example.com/book2\">Buy</a>\n",
      "</div>]\n"
     ]
    }
   ],
   "source": [
    "# 2. Find by class\n",
    "first_product = example_soup.find(\"div\", class_=\"product\")\n",
    "print(first_product)\n",
    "print('--------------------')\n",
    "all_products = example_soup.find_all(\"div\", class_=\"product\")\n",
    "print(all_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e7fc014-1b78-43a3-910b-cce8d0287b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"product\" id=\"p1\">\n",
      "<h2 class=\"title\">Book One</h2>\n",
      "<span class=\"price\">$19.99</span>\n",
      "<a class=\"buy-link\" href=\"https://example.com/book1\">Buy</a>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# 3. Find by id\n",
    "product_p1 = example_soup.find(\"div\", id=\"p1\")\n",
    "print(product_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f40d4a10-8264-42bb-8655-23256e7de680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"buy-link\" href=\"https://example.com/book1\">Buy</a>\n",
      "--------------------\n",
      "[<img alt=\"Cover 1\" src=\"cover1.jpg\"/>, <img alt=\"Cover 2\" src=\"cover2.jpg\"/>]\n"
     ]
    }
   ],
   "source": [
    "# 4. Find by other attributes\n",
    "first_link = example_soup.find(\"a\", href=\"https://example.com/book1\")\n",
    "print(first_link)\n",
    "print('--------------------')\n",
    "all_images = example_soup.find_all(\"img\", src=True)\n",
    "print(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141f490-bbb6-472e-980d-4cd69b62b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Nested find\n",
    "product_price = first_product.find(\"span\", class_=\"price\").text\n",
    "print(product_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e4626-faa4-4a03-ae2f-5e4dd65777aa",
   "metadata": {},
   "source": [
    "**Using `.find()` and `.find_all()` in BeautifulSoup**\n",
    "\n",
    "BeautifulSoup provides two main methods for locating elements in an HTML document:\n",
    "\n",
    "- **`.find()`** → returns the **first matching element**.  \n",
    "- **`.find_all()`** → returns a **list of all matching elements**.\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80743aa8-c16a-405e-8de6-60b33a7bf441",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce123c03-3281-48ca-b1de-4cd1abfb25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find(id=\"ResultsContainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff781b6-c56d-4579-96ce-e7532a675847",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_cards = results.find_all(\"div\", class_=\"card-content\")\n",
    "# Lets look at the first job card\n",
    "print(job_cards[0].prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e73ada2-a883-4014-89a2-32e2843d1a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = dict()\n",
    "\n",
    "for i,job_card in enumerate(job_cards):\n",
    "    # Get the information from each job card\n",
    "    title_element = job_card.find(\"h2\", class_=\"title\")\n",
    "    company_element = job_card.find(\"h3\", class_=\"company\")\n",
    "    location_element = job_card.find(\"p\", class_=\"location\")\n",
    "    # Add the information to the dictionary\n",
    "    jobs[i] = {'job':title_element.text.strip(),\n",
    "               'company':company_element.text.strip(),\n",
    "                'location':location_element.text.strip()}\n",
    "\n",
    "df = pd.DataFrame(jobs).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1868a9-2033-44b6-b7a9-14ac3f9245f3",
   "metadata": {},
   "source": [
    "### Extracting Attributes\n",
    "\n",
    "We have already seen how to extract text from one of our elements. The text is just one part of the html code. Sometimes we want other parts of the content. For example, we may want to know about some of the attributes that come before the text. Attributes are not typed out to the website, but rather act on the text or help organize the elements on the page.\n",
    "\n",
    "### Common HTML Attributes\n",
    "\n",
    "HTML attributes provide additional information about elements and are often used in web scraping to locate or filter data. Some of the most common attributes include:\n",
    "\n",
    "- **id**  \n",
    "  - Uniquely identifies an element on the page.  \n",
    "  - Example: `<div id=\"main-content\">`\n",
    "\n",
    "- **class**  \n",
    "  - Groups elements for styling or scripting. Multiple elements can share the same class.  \n",
    "  - Example: `<span class=\"price\">`\n",
    "\n",
    "- **href**  \n",
    "  - Used in `<a>` tags to specify the link destination.  \n",
    "  - Example: `<a href=\"https://example.com\">Visit</a>`\n",
    "\n",
    "- **src**  \n",
    "  - Specifies the source for media elements like images, videos, or scripts.  \n",
    "  - Example: `<img src=\"cover.jpg\" alt=\"Book Cover\">`\n",
    "\n",
    "- **alt**  \n",
    "  - Provides alternative text for images, shown if the image cannot load.  \n",
    "  - Example: `<img src=\"cover.jpg\" alt=\"Book Cover\">`\n",
    "\n",
    "- **title**  \n",
    "  - Gives additional information about an element, usually shown as a tooltip.  \n",
    "  - Example: `<a href=\"#\" title=\"Click here for more info\">Link</a>`\n",
    "\n",
    "- **style**  \n",
    "  - Inline CSS styling for an element.  \n",
    "  - Example: `<p style=\"color:red;\">Important text</p>`\n",
    "\n",
    "- **name**  \n",
    "  - Often used in form elements to identify input fields.  \n",
    "  - Example: `<input type=\"text\" name=\"username\">`\n",
    "\n",
    "- **type**  \n",
    "  - Specifies the type of input or button in forms.  \n",
    "  - Example: `<input type=\"password\">`\n",
    "\n",
    "- **target**  \n",
    "  - Defines where to open linked documents (new tab, same tab, etc.).  \n",
    "  - Example: `<a href=\"https://example.com\" target=\"_blank\">Visit</a>`\n",
    "\n",
    "- **data-* (custom data attributes)**  \n",
    "  - Custom attributes used to store extra information on elements.  \n",
    "  - Example: `<div data-id=\"123\" data-category=\"books\">`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654e466-171b-4aad-8dec-ccee6e3bce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one\n",
    "job_card = job_cards[0]\n",
    "title_element = job_card.find(\"h2\", class_=\"title\")\n",
    "print(title_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4325dcd-77b3-4063-ac82-1bdafe957cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the information about the class\n",
    "title_element['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c637618-9145-40e0-a9ff-e1557248c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the example code above: get the first link\n",
    "link_example = example_soup.find('a')\n",
    "link_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da3a16-11e1-44e9-851e-7a8252b481fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the text\n",
    "link_example.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f943885-a65b-4104-badc-b515e8c11a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what class it belongs to\n",
    "link_example['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b23146-8e9e-4aa7-be69-f8e7008a0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the url for the link\n",
    "link_example['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde42b6-44ab-470a-a77d-bcc616d90fba",
   "metadata": {},
   "source": [
    "## You Try\n",
    "\n",
    "Extract the application link for each of the jobs and add it to the data frame. Save this information to a list and then add it to the data frame as a column named \"application link\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7aec93-dc42-4af3-88b7-15cbe7b094e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "links = []\n",
    "for i,job_card in enumerate(job_cards):\n",
    "    # Get the information from each job card\n",
    "    links.append(job_card.find(\"a\", class_=\"card-footer-item\"))\n",
    "\n",
    "links[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c2262d-ce8d-4867-9ab7-05e590ceeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one\n",
    "job_card = job_cards[0]\n",
    "title_element = job_card.find(\"h2\", class_=\"title\")\n",
    "print(title_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d72a0-3744-44e2-8398-8f16a7fd282f",
   "metadata": {},
   "source": [
    "## More advanced Parsing\n",
    "\n",
    "### Functions and Beautiful Soup\n",
    "\n",
    "You can sometimes leverage functions inside the beautiful soup method to extract things in a more pythonic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52f5504-e0ca-450f-92ba-7a549edec977",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_jobs = results.find_all(\"h2\", \n",
    "                               string=lambda x: \"python\" in x.lower())\n",
    "\n",
    "python_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de97874-4c91-4c32-aa4a-f10fcda4ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in python_jobs:\n",
    "    print(job.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7318a-a550-4f56-965e-a466fb74b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = python_jobs[0]\n",
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c30980-feae-4045-99d5-6d85f27503c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why does this give an error?\n",
    "title_element = job.find(\"h2\", class_=\"title\")\n",
    "title_element.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6298cf8b-a531-4d37-a540-6b71f619dce8",
   "metadata": {},
   "source": [
    "### Searching Hierarchy\n",
    "\n",
    "You can also search starting from an interior element. Here job is just one \"h2\" element, but we can look at its parent containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b4061-6042-418d-a417-924c20b1ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look up through the hierarchy to get additional information\n",
    "print(job)\n",
    "print('--------------------')\n",
    "print(job.parent)\n",
    "print('--------------------')\n",
    "print(job.parent.parent)\n",
    "print('--------------------')\n",
    "print(job.parent.parent.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1a22e-02da-45ae-ba07-232fdc579a74",
   "metadata": {},
   "source": [
    "## You Try\n",
    "\n",
    "Try to scrape the quotes and authors from this website:\n",
    "\n",
    "https://quotes.toscrape.com/\n",
    "\n",
    "- what happens to the url when you push the next button at the bottom of the page? \n",
    "- what happens to the url when you click on a tag?\n",
    "\n",
    "Try to scrape all the quotes for one of the larger tags: love, inspirational, life, humor, or books. Make sure to get all the pages!\n",
    "\n",
    "Put this information into a DataFrame.\n",
    "\n",
    "Challenge (optional) - Scrape all the quotes on the site along with authors names and tags for each quote. Put all of this into a DataFrame and do an analysis. Who has the most quotes, longest or shortest quotes, most love quotes, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf42994-64dc-4e23-8a4a-025908f37e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91645c7c-1136-4f6a-9fdf-95967419e631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268bd174-6894-488e-8731-72c4494090ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e9e5940-7f09-4e95-9c74-5c8492773201",
   "metadata": {},
   "source": [
    "## Dynamic Web Scraping Process Using Selenium\n",
    "\n",
    "Dynamic websites load content using JavaScript, or other dynamic processes. Unlike static pages, the HTML source may initially contain little data, requiring tools like **Selenium** to interact with the page and extract information.\n",
    "\n",
    "Prepare the driver/browser\n",
    "\n",
    "1. We need to install selenium and webdriver-manager\n",
    "2. Then we import all the packages - my example is using chrome\n",
    "3. Choose options that work for your computer\n",
    "\n",
    "Now we are ready to scrape\n",
    "\n",
    "1. Start the driver\n",
    "2. Get the html - waiting for the JavaScript to load\n",
    "3. Execute the script to get the HTML\n",
    "4. Use Beautiful Soup to parse the HTML\n",
    "5. Close the browser\n",
    "\n",
    "**Key Benefit:** Selenium allows scraping of content that is not present in the initial HTML source, making it ideal for modern, JavaScript-heavy websites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae0caff-bda0-453c-bf33-3aa1351e0a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try our Dynamic site\n",
    "URL_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107a042-4073-44a3-bfa7-ca1e636cdc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while loading conda entry point: conda-libmamba-solver (dlopen(/Users/sethchairez/anaconda3/lib/python3.12/site-packages/libmambapy/bindings.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libarchive.19.dylib\n",
      "  Referenced from: <DABAAA67-C742-3F02-A1E7-430216CA1374> /Users/sethchairez/anaconda3/lib/libmamba.2.0.0.dylib\n",
      "  Reason: tried: '/Users/sethchairez/anaconda3/lib/libarchive.19.dylib' (no such file), '/Users/sethchairez/anaconda3/lib/python3.12/site-packages/libmambapy/../../../libarchive.19.dylib' (no such file), '/Users/sethchairez/anaconda3/lib/python3.12/site-packages/libmambapy/../../../libarchive.19.dylib' (no such file), '/Users/sethchairez/anaconda3/bin/../lib/libarchive.19.dylib' (no such file), '/Users/sethchairez/anaconda3/bin/../lib/libarchive.19.dylib' (no such file), '/usr/local/lib/libarchive.19.dylib' (no such file), '/usr/lib/libarchive.19.dylib' (no such file, not in dyld cache))\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Solving environment: unsuccessful attempt using repodata from current_repodata.json, retrying with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Solving environment: \\ "
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge -y selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "116ef570-7a09-4f66-a356-819a313f2853",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now we need to import a lot of packages!\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# I am using chrome here - you need to already have chrome installed\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Service\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Options\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "# Now we need to import a lot of packages!\n",
    "# I am using chrome here - you need to already have chrome installed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48482164-8549-4ffa-b36e-52e89f8d31ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Chrome options\n",
    "# These are the options that I needed for my computer\n",
    "# I would keep --headless or --headless=new\n",
    "# The others you could probably comment out!\n",
    "options = Options()\n",
    "#options.add_argument(\"--headless=new\") # So a browser window doesnt open\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--remote-debugging-port=9222\")\n",
    "\n",
    "# Annoyingly I had to specify an exact driver version\n",
    "# Driver manager should figure this out for me, but it didn't\n",
    "# You should be able to just run: service = Service(ChromeDriverManager().install())\n",
    "service = Service(ChromeDriverManager(driver_version=\"140.0.7339.185\").install())\n",
    "driver = webdriver.Chrome(service=service,options=options)\n",
    "\n",
    "# Use the driver to get the webpage\n",
    "driver.get(URL_dynamic)\n",
    "\n",
    "# Wait for JavaScript to load\n",
    "#time.sleep(3)\n",
    "\n",
    "# Extract content (stuff on the page)\n",
    "html_source_code = driver.execute_script(\"return document.body.innerHTML;\")\n",
    "# Save to beautiful soup\n",
    "soup = BeautifulSoup(html_source_code, 'html.parser')\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe556b64-cbcf-4e5c-a4bb-13eee1ea6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can extract the information\n",
    "cats = soup.find_all('div', class_=\"tsingle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1da806-84b7-4a80-9af4-77509cf086f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I really need to see some cute cat photos!\n",
    "for cat in cats:\n",
    "    photo = cat.find('a')\n",
    "    print('https://en.wikipedia.org/'+photo['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22dda5e-13ff-41f4-9b5c-1f7a5a485e3a",
   "metadata": {},
   "source": [
    "## Practice! Practice! Practice!\n",
    "\n",
    "The more you practice web scraping the better you will get at dealing with the curveballs that get thrown your way. Here are some great places to try out your skills\n",
    "\n",
    "https://www.scrapethissite.com/pages/\n",
    "\n",
    "https://webscraper.io/test-sites\n",
    "\n",
    "https://realpython.github.io/fake-jobs/\n",
    "\n",
    "https://books.toscrape.com/\n",
    "\n",
    "https://quotes.toscrape.com/\n",
    "\n",
    "https://www.wikipedia.org/ - NOT A FAKE SITE - but good for practice. Please follow their rules for scraping: https://wikitech.wikimedia.org/wiki/Robot_policy, basically this means don't send too many requests, if you are looking for large amounts of data use the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f00063-4ce6-49f3-85ac-e5a09fb32499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
